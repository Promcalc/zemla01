import os
import re
import time
import json
import feedparser
import requests
import urllib3
from datetime import datetime
from html import unescape

import gspread
from google.oauth2.service_account import Credentials

from dotenv import load_dotenv

# –û—Ç–∫–ª—é—á–∞–µ–º SSL-–ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏—è
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

load_dotenv()  # –∑–∞–≥—Ä—É–∂–∞–µ—Ç –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –∏–∑ .env

# === –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è ===
RSS_URL = "https://torgi.gov.ru/new/api/public/lotcards/rss?lotStatus=PUBLISHED,APPLICATIONS_SUBMISSION&catCode=2&byFirstVersion=true"
MAP_URL = "https://nspd.gov.ru/map?thematic=PKK&zoom=14.022938145428002&coordinate_x=10153878.513581853&coordinate_y=7361695.523330088&baseLayerId=235&theme_id=1&is_copy_url=true"
GEO_API_BASE = "https://nspd.gov.ru/api/geoportal/v2/search/geoportal"
SHEET_ID = os.environ["GOOGLE_SHEET_ID"]

# –ù–æ–≤—ã–π –∑–∞–≥–æ–ª–æ–≤–æ–∫
LOT_INFO_COL = "Lot_info"

USER_AGENT = "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/139.0.0.0 Safari/537.36 OPR/123.0.0.0 (Edition Yx 05)"

MAX_CELL_CHARS = 50000

# ‚úÖ –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã–π —Ä–µ–≥—ç–∫—Å–ø: –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç 4‚Äì19 —Ü–∏—Ñ—Ä –≤ —Ç—Ä–µ—Ç—å–µ–π —á–∞—Å—Ç–∏ (–∫–≤–∞—Ä—Ç–∞–ª+—É—á–∞—Å—Ç–æ–∫)
CADASTRAL_PATTERN = re.compile(r'\b\d{2}:\d{2}:\d{4,19}:\d{1,6}\b')

# === –í–°–ü–û–ú–û–ì–ê–¢–ï–õ–¨–ù–´–ï –§–£–ù–ö–¶–ò–ò ===

def validate_and_truncate_row(row: list, headers: list, row_index_in_batch: int, lot_id: str = "") -> list:
    """
    –ü—Ä–æ–≤–µ—Ä—è–µ—Ç –∫–∞–∂–¥—É—é —è—á–µ–π–∫—É –≤ —Å—Ç—Ä–æ–∫–µ –Ω–∞ –ø—Ä–µ–≤—ã—à–µ–Ω–∏–µ –ª–∏–º–∏—Ç–∞ Google Sheets (50k —Å–∏–º–≤–æ–ª–æ–≤).
    –ï—Å–ª–∏ –ø—Ä–µ–≤—ã—à–∞–µ—Ç ‚Äî –∑–∞–º–µ–Ω—è–µ—Ç –Ω–∞ –ø—É—Å—Ç—É—é —Å—Ç—Ä–æ–∫—É –∏ –≤—ã–≤–æ–¥–∏—Ç –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ.
    """
    validated_row = []
    for col_idx, cell_value in enumerate(row):
        cell_str = str(cell_value) if cell_value is not None else ""
        if len(cell_str) > MAX_CELL_CHARS:
            field_name = headers[col_idx] if col_idx < len(headers) else f"Column_{col_idx}"
            preview = cell_str.replace("\n", "\\n")
            print(f"‚ö†Ô∏è CELL TOO LONG (row {row_index_in_batch}, lot '{lot_id}')")
            print(f"   Field: {field_name}")
            print(f"   Length: {len(cell_str)} chars (max {MAX_CELL_CHARS})")
            print(f"   Preview: {preview}")
            validated_row.append("")  # —Å–æ—Ö—Ä–∞–Ω—è–µ–º –ø—É—Å—Ç–æ, —á—Ç–æ–±—ã –Ω–µ —Å–ª–æ–º–∞—Ç—å –≤—Å—Ç–∞–≤–∫—É
        else:
            validated_row.append(cell_value)
    return validated_row

# –ò–∑–≤–ª–µ–∫–∞–µ—Ç ID –ª–æ—Ç–∞ –∏–∑ —Å—Å—ã–ª–∫–∏
def extract_lot_id_from_link(link: str) -> str:
    """–ò–∑ 'https://torgi.gov.ru/.../23000030610000000997_1  ' ‚Üí '23000030610000000997_1'"""
    if not link:
        return ""
    # –£–±–∏—Ä–∞–µ–º –ø—Ä–æ–±–µ–ª—ã –≤ –∫–æ–Ω—Ü–µ
    link = link.strip()
    # –ë–µ—Ä—ë–º —á–∞—Å—Ç—å –ø–æ—Å–ª–µ –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ '/'
    parts = link.rstrip('/').split('/')
    if parts:
        lot_id = parts[-1]
        # –û—Å—Ç–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ —Ü–∏—Ñ—Ä—ã, –ø–æ–¥—á—ë—Ä–∫–∏–≤–∞–Ω–∏—è
        lot_id_clean = re.sub(r'[^0-9_]', '', lot_id)
        if lot_id_clean:
            return lot_id_clean
    return ""

# –ü–æ–ª—É—á–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –ª–æ—Ç–∞ –ø–æ ID
def fetch_lot_info(lot_id: str, referer: str):
    """–ó–∞–ø—Ä–∞—à–∏–≤–∞–µ—Ç –¥–µ—Ç–∞–ª–∏ –ª–æ—Ç–∞ —Å torgi.gov.ru"""
    if not lot_id:
        return None

    url = f"https://torgi.gov.ru/new/api/public/lotcards/{lot_id}"
    headers = {
        "Accept": "application/json, text/plain, */*",
        "Accept-Language": "ru-RU,ru;q=0.9,en-US;q=0.8,en;q=0.7",
        "Connection": "keep-alive",
        "Referer": referer,
        "Sec-Fetch-Dest": "empty",
        "Sec-Fetch-Mode": "cors",
        "Sec-Fetch-Site": "same-origin",
        "User-Agent": USER_AGENT,
        "branchId": "null",
        "organizationId": "null",
        "sec-ch-ua": '"Not;A=Brand";v="99", "Opera";v="123", "Chromium";v="139"',
        "sec-ch-ua-mobile": "?0",
        "sec-ch-ua-platform": '"Windows"',
        "traceparent": "00-4028d76347b5b5ea5b4479f015343701-e346b4143d2840ea-01"
    }

    try:
        resp = requests.get(url, headers=headers, verify=False, timeout=15)
        if resp.status_code == 200:
            return resp.json()
        else:
            print(f"‚ö†Ô∏è Lot info error {resp.status_code} for {lot_id}")
            return None
    except Exception as e:
        print(f"üí• Lot info exception for {lot_id}: {e}")
        return None

def clean_html_tags(text: str) -> str:
    if not text:
        return ""
    clean = re.sub(r'<[^>]+>', '', text)
    clean = unescape(clean)
    return clean.strip()

def normalize_field_name(name: str) -> str:
    name = name.strip()
    if name.endswith(':'):
        name = name[:-1].strip()
    name = name.replace(':', '_')
    name = re.sub(r'\s+', ' ', name)
    return name.capitalize()

def parse_description_fields(description_html: str) -> dict:
    if not description_html:
        return {}
    partially_clean = unescape(description_html)
    parts = re.split(r'<br\s*/?>', partially_clean, flags=re.IGNORECASE)
    fields = {}
    for part in parts:
        clean_part = clean_html_tags(part)
        clean_part = clean_part.strip()
        if not clean_part or ':' not in clean_part:
            continue
        key_raw, value_raw = clean_part.split(':', 1)
        key_norm = normalize_field_name(key_raw)
        value_clean = value_raw.strip()
        if key_norm and value_clean:
            fields[key_norm] = value_clean
    return fields

def extract_item_raw_fields(item) -> dict:
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–µ –ø–æ–ª—è –∏–∑ feedparser-—ç–ª–µ–º–µ–Ω—Ç–∞."""
    fields = {}
    # –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ –ø–æ–ª—è
    if hasattr(item, 'title') and item.title:
        fields['title'] = item.title
    if hasattr(item, 'link') and item.link:
        fields['link'] = item.link
    if hasattr(item, 'description') and item.description:
        fields['description'] = item.description
#    if hasattr(item, 'published') and item.published:
#        fields['pubDate'] = item.published
    # ‚úÖ –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º pubDate –≤ ISO
    if hasattr(item, 'published') and item.published and item.published_parsed:
        try:
            dt = datetime.fromtimestamp(time.mktime(item.published_parsed))
            fields['pubDate'] = dt.isoformat()  # ‚Üê ISO-—Ñ–æ—Ä–º–∞—Ç!
        except:
            fields['pubDate'] = item.published  # fallback
    elif hasattr(item, 'published'):
        fields['pubDate'] = item.published

    if hasattr(item, 'id') and item.id:
        fields['guid'] = item.id

    # –ü–æ–ª—è –∏–∑ namespaces (dc:date –∏ –¥—Ä.)
    if hasattr(item, 'dc_date') and item.dc_date:
        fields['dc:date'] = item.dc_date
    elif 'dc' in item and 'date' in item['dc']:
        dc_val = item['dc']['date']
        if isinstance(dc_val, list) and dc_val:
            fields['dc:date'] = dc_val[0]
        elif isinstance(dc_val, str):
            fields['dc:date'] = dc_val

    return fields

def extract_cadastral_number_from_item(item_fields: dict, desc_fields: dict) -> str:
    """–ò—â–µ—Ç –∫–∞–¥–∞—Å—Ç—Ä–æ–≤—ã–π –Ω–æ–º–µ—Ä —Å–Ω–∞—á–∞–ª–∞ –≤ –ø–æ–ª—è—Ö, –ø–æ—Ç–æ–º –≤ —Ç–µ–∫—Å—Ç–µ."""
    # 1. –í —Ä–∞—Å–ø–∞—Ä—Å–µ–Ω–Ω—ã—Ö –ø–æ–ª—è—Ö description
    for key, value in desc_fields.items():
        if "–∫–∞–¥–∞—Å—Ç—Ä–æ–≤—ã–π –Ω–æ–º–µ—Ä" in key.lower():
            if CADASTRAL_PATTERN.fullmatch(value.strip()):
                return value.strip()

    # 2. –í –ø–æ–ª—è—Ö item (–Ω–∞ —Å–ª—É—á–∞–π, –µ—Å–ª–∏ —É–∂–µ –µ—Å—Ç—å –∫–∞–∫ –æ—Ç–¥–µ–ª—å–Ω–æ–µ –ø–æ–ª–µ)
    for key, value in item_fields.items():
        if "–∫–∞–¥–∞—Å—Ç—Ä–æ–≤—ã–π –Ω–æ–º–µ—Ä" in key.lower():
            if CADASTRAL_PATTERN.fullmatch(str(value).strip()):
                return str(value).strip()

    # 3. –í –æ–±—â–µ–º —Ç–µ–∫—Å—Ç–µ (title + description)
    text = item_fields.get("title", "") + " " + item_fields.get("description", "")
    match = CADASTRAL_PATTERN.search(text)
    return match.group(0) if match else ""

def get_session_with_cookies():
    session = requests.Session()
    session.verify = False
    session.headers.update({
        "user-agent": USER_AGENT,
        "accept-language": "ru-RU,ru;q=0.9,en-US;q=0.8,en;q=0.7",
    })
    resp = session.get(MAP_URL, timeout=10)
    resp.raise_for_status()
    return session

def format_error_response(response):
    try:
        body = response.text[:10000]
    except:
        body = "<binary or unreadable>"
    return (
        f"Status: {response.status_code}\n"
        f"URL: {response.url}\n"
        f"Headers: {dict(response.headers)}\n"
        f"Body: {body}"
    )

def fetch_geoportal_data(session, cad_num):
    url = f"{GEO_API_BASE}?thematicSearchId=1&query={requests.utils.quote(cad_num)}"
    headers = {
        "accept": "*/*",
        "referer": MAP_URL,
        "sec-ch-ua": '"Not;A=Brand";v="99", "Opera";v="123", "Chromium";v="139"',
        "sec-ch-ua-mobile": "?0",
        "sec-ch-ua-platform": '"Windows"',
        "sec-fetch-dest": "empty",
        "sec-fetch-mode": "cors",
        "sec-fetch-site": "same-origin",
        "priority": "u=1, i",
    }
    try:
        resp = session.get(url, headers=headers, timeout=15)
        if resp.status_code == 200:
            data = resp.json()
            return data, None
        else:
            return None, format_error_response(resp)
    except Exception as e:
        return None, f"Exception: {str(e)}"

def get_sheet():
    creds_json = json.loads(os.environ["GOOGLE_CREDENTIALS"])
    scopes = ["https://www.googleapis.com/auth/spreadsheets"]
    creds = Credentials.from_service_account_info(creds_json, scopes=scopes)
    client = gspread.authorize(creds)
    return client.open_by_key(SHEET_ID).sheet1

def collect_all_field_names_from_items(items):
    field_set = set()
    for item in items:
        item_fields = extract_item_raw_fields(item)
        for key, value in item_fields.items():
            if isinstance(value, str):
                field_set.add(normalize_field_name(key))
        desc_fields = parse_description_fields(item_fields.get("description", ""))
        field_set.update(desc_fields.keys())
    special_fields = {"–ö–∞–¥–∞—Å—Ç—Ä–æ–≤—ã–π –Ω–æ–º–µ—Ä", "Nspd_data", "Nspd_error", "Unsorted", LOT_INFO_COL}
    field_set.update(special_fields)
    sorted_fields = sorted([f for f in field_set if f != "Unsorted"])
    sorted_fields.append("Unsorted")
    return sorted_fields

def build_row_for_sheet(item_fields, desc_fields, headers, cadastral_number="", nspd_data="", nspd_error=""):
    row_dict = {}
    for key, value in item_fields.items():
        if isinstance(value, str):
            field_name = normalize_field_name(key)
            row_dict[field_name] = value
    row_dict.update(desc_fields)
    row_dict["–ö–∞–¥–∞—Å—Ç—Ä–æ–≤—ã–π –Ω–æ–º–µ—Ä"] = cadastral_number
    row_dict["Nspd_data"] = nspd_data
    row_dict["Nspd_error"] = nspd_error

    header_to_index = {name: i for i, name in enumerate(headers)}
    row = [""] * len(headers)
    unsorted_pairs = []
    for field_name, value in row_dict.items():
        if field_name in header_to_index:
            row[header_to_index[field_name]] = str(value) if value is not None else ""
        else:
            unsorted_pairs.append(f"{field_name}: {value}")
    if "Unsorted" in header_to_index:
        row[header_to_index["Unsorted"]] = "\n".join(unsorted_pairs)
    return row

def find_last_filled_row_in_column(sheet, col_letter: str, max_rows_limit: int = 100000) -> int:
    """
    –ù–∞—Ö–æ–¥–∏—Ç –Ω–æ–º–µ—Ä –ø–æ—Å–ª–µ–¥–Ω–µ–π –Ω–µ–ø—É—Å—Ç–æ–π —Å—Ç—Ä–æ–∫–∏ –≤ —É–∫–∞–∑–∞–Ω–Ω–æ–π –∫–æ–ª–æ–Ω–∫–µ Google –¢–∞–±–ª–∏—Ü—ã.
    
    –ê–ª–≥–æ—Ä–∏—Ç–º:
      1. –≠–∫—Å–ø–æ–Ω–µ–Ω—Ü–∏–∞–ª—å–Ω—ã–π —Ä–æ—Å—Ç: –ø—Ä–æ–≤–µ—Ä—è–µ–º —Å—Ç—Ä–æ–∫–∏ 1, 2, 4, 8, 16, ..., –ø–æ–∫–∞ –Ω–µ –Ω–∞–π–¥—ë–º –ø—É—Å—Ç—É—é.
      2. –ë–∏–Ω–∞—Ä–Ω—ã–π –ø–æ–∏—Å–∫ –º–µ–∂–¥—É –ø–æ—Å–ª–µ–¥–Ω–µ–π –∑–∞–ø–æ–ª–Ω–µ–Ω–Ω–æ–π –∏ –ø–µ—Ä–≤–æ–π –ø—É—Å—Ç–æ–π.
    
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:
      - –ù–æ–º–µ—Ä —Å—Ç—Ä–æ–∫–∏ (int), –µ—Å–ª–∏ –Ω–∞–π–¥–µ–Ω–∞ —Ö–æ—Ç—è –±—ã –æ–¥–Ω–∞ –Ω–µ–ø—É—Å—Ç–∞—è —Å—Ç—Ä–æ–∫–∞ (–Ω–∞—á–∏–Ω–∞—è —Å 2, —Ç.–∫. 1 ‚Äî –∑–∞–≥–æ–ª–æ–≤–∫–∏)
      - 0, –µ—Å–ª–∏ –Ω–µ—Ç –Ω–∏ –æ–¥–Ω–æ–π –Ω–µ–ø—É—Å—Ç–æ–π —Å—Ç—Ä–æ–∫–∏ –ø–æ—Å–ª–µ –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤
    """
    low = 1  # –ø–µ—Ä–≤–∞—è —Å—Ç—Ä–æ–∫–∞ ‚Äî –∑–∞–≥–æ–ª–æ–≤–∫–∏, –Ω–∞—Å –∏–Ω—Ç–µ—Ä–µ—Å—É–µ—Ç –Ω–∞—á–∏–Ω–∞—è —Å–æ 2
    high = 1

    # –®–∞–≥ 1: –≠–∫—Å–ø–æ–Ω–µ–Ω—Ü–∏–∞–ª—å–Ω—ã–π —Ä–æ—Å—Ç, –ø–æ–∫–∞ –Ω–µ –Ω–∞–π–¥—ë–º –ø—É—Å—Ç—É—é —Å—Ç—Ä–æ–∫—É
    while high <= max_rows_limit:
        range_name = f"{col_letter}{high}:{col_letter}{high}"
        try:
#            print(range_name)
            values = sheet.get(range_name)
            if not values or not values[0] or not values[0][0].strip():
                # –ù–∞—à–ª–∏ –ø—É—Å—Ç—É—é —Å—Ç—Ä–æ–∫—É ‚Üí –æ—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º—Å—è
                break
        except Exception:
            # –°—á–∏—Ç–∞–µ–º –ø—É—Å—Ç–æ–π
            break
        low = high
        high *= 2

    # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º —Å–≤–µ—Ä—Ö—É
    high = min(high, max_rows_limit)

    # –®–∞–≥ 2: –ë–∏–Ω–∞—Ä–Ω—ã–π –ø–æ–∏—Å–∫ –º–µ–∂–¥—É low –∏ high
    last_filled = 0
    while low <= high:
        mid = (low + high) // 2
        range_name = f"{col_letter}{mid}:{col_letter}{mid}"
        try:
            values = sheet.get(range_name)
            if values and values[0] and values[0][0].strip():
                last_filled = mid
                low = mid + 1
            else:
                high = mid - 1
        except Exception:
            high = mid - 1

    # –ù–∞—Å –∏–Ω—Ç–µ—Ä–µ—Å—É—é—Ç —Ç–æ–ª—å–∫–æ —Å—Ç—Ä–æ–∫–∏ –ø–æ—Å–ª–µ –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤ (>=2)
    return last_filled if last_filled >= 2 else 0

def parse_date_flexible(date_str: str):
    """–ü–∞—Ä—Å–∏—Ç –¥–∞—Ç—É –≤ ISO –∏–ª–∏ RFC-2822 —Ñ–æ—Ä–º–∞—Ç–µ."""
    if not date_str:
        return None
    # –°–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–±—É–µ–º ISO
    try:
        return datetime.fromisoformat(date_str.replace("Z", "+00:00"))
    except:
        pass
    # –ü–æ—Ç–æ–º RFC-2822
    try:
        import email.utils
        return datetime.fromtimestamp(email.utils.parsedate_to_datetime(date_str).timestamp())
    except:
        return None

# === –û–°–ù–û–í–ù–ê–Ø –õ–û–ì–ò–ö–ê ===

def main():
    sheet = get_sheet()

    # ================================
    # üîÅ –û–ë–†–ê–ë–û–¢–ö–ê –°–£–©–ï–°–¢–í–£–Æ–©–ò–• –ó–ê–ü–ò–°–ï–ô (–¥–ª—è –ø–µ—Ä–≤–æ–≥–æ –ø—Ä–æ–≥–æ–Ω–∞ –ü–û–°–õ–ï –¥–æ–±–∞–≤–ª–µ–Ω–∏—è Lot_info)
    # –ü–æ—Å–ª–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è ‚Äî –ó–ê–ö–û–ú–ú–ï–ù–¢–ò–†–£–ô–¢–ï —ç—Ç–æ—Ç –±–ª–æ–∫!
    # ================================
#    try:
#        first_row = sheet.row_values(1)
#        if LOT_INFO_COL in first_row:
#            print("üîß Found existing table with Lot_info column. Processing old rows...")
#            link_col_idx = None
#            lot_info_col_idx = None
#            for i, name in enumerate(first_row):
#                if normalize_field_name("link") == normalize_field_name(name):
#                    link_col_idx = i
#                if name == LOT_INFO_COL:
#                    lot_info_col_idx = i
#
#        if link_col_idx is not None and lot_info_col_idx is not None:
#            last_row = find_last_filled_row_in_column(sheet, gspread.utils.rowcol_to_a1(1, link_col_idx + 1)[0])
#            if last_row > 0:
#                print(f"üßÆ Processing rows 2 to {last_row} for Lot_info (in batches)...")
#                
#                # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –±—É–∫–≤—ã –∫–æ–ª–æ–Ω–æ–∫
#                link_col_letter = gspread.utils.rowcol_to_a1(1, link_col_idx + 1)[0]
#                lot_info_col_letter = gspread.utils.rowcol_to_a1(1, lot_info_col_idx + 1)[0]
#
#                batch_size = 30  # ‚â§30 ‚Äî –±–µ–∑–æ–ø–∞—Å–Ω–æ –¥–ª—è –ª–∏–º–∏—Ç–æ–≤
#                start_row = 2
#                while start_row <= last_row:
#                    end_row = min(start_row + batch_size - 1, last_row)
#                    print(f"  üì• Reading rows {start_row}‚Äì{end_row}...")
#
#                    # –ü–∞–∫–µ—Ç–Ω–æ–µ —á—Ç–µ–Ω–∏–µ
#                    link_range = f"{link_col_letter}{start_row}:{link_col_letter}{end_row}"
#                    lot_info_range = f"{lot_info_col_letter}{start_row}:{lot_info_col_letter}{end_row}"
#                    
#                    try:
#                        batch_data = sheet.batch_get([link_range, lot_info_range])
#                        links_batch = batch_data[0] if len(batch_data) > 0 else []
#                        lot_info_batch = batch_data[1] if len(batch_data) > 1 else []
#                    except Exception as e:
#                        print(f"    ‚ùå Batch read error: {e}")
#                        time.sleep(10)
#                        start_row += batch_size
#                        continue
#
#                    # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –ø–∞–∫–µ—Ç
#                    updates = []
#                    for i in range(len(links_batch)):
#                        row_num = start_row + i
#                        link_val = links_batch[i][0] if i < len(links_batch) and links_batch[i] else ""
#                        lot_info_val = lot_info_batch[i][0] if i < len(lot_info_batch) and lot_info_batch[i] else ""
#
#                        if link_val and (not lot_info_val or lot_info_val.strip() == ""):
#                            lot_id = extract_lot_id_from_link(link_val)
#                            if lot_id:
#                                print(f"    üì• Fetching lot info for {lot_id} (row {row_num})")
#                                lot_data = fetch_lot_info(lot_id, link_val)
#                                if lot_data:
#                                    cell_addr = gspread.utils.rowcol_to_a1(row_num, lot_info_col_idx + 1)
#                                    updates.append({
#                                        "range": cell_addr,
#                                        "values": [[json.dumps(lot_data, ensure_ascii=False)]]
#                                    })
#                                time.sleep(0.3)  # –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏ –∫ torgi.gov.ru
#
#                    # –ü–∞–∫–µ—Ç–Ω–∞—è –∑–∞–ø–∏—Å—å (–µ—Å–ª–∏ –µ—Å—Ç—å —á—Ç–æ –æ–±–Ω–æ–≤–ª—è—Ç—å)
#                    if updates:
#                        try:
#                            sheet.batch_update(updates)
#                            print(f"    ‚úÖ Updated {len(updates)} rows")
#                        except Exception as e:
#                            print(f"    ‚ùå Batch update error: {e}")
#                            time.sleep(5)
#
#                    # –ó–∞–¥–µ—Ä–∂–∫–∞ –ø–µ—Ä–µ–¥ —Å–ª–µ–¥—É—é—â–∏–º –ø–∞–∫–µ—Ç–æ–º
#                    time.sleep(2.0)
#                    start_row += batch_size
#            else:
#                print("‚ö†Ô∏è Could not find Link or Lot_info columns")
#        else:
#            print("üÜï Lot_info column not present ‚Äî skipping old rows processing")
#    except Exception as e:
#        print(f"‚ö†Ô∏è Old rows processing failed: {e}")

    # ================================
    # –ö–û–ù–ï–¶ –ë–õ–û–ö–ê –î–õ–Ø –ó–ê–ö–æ–º–ú–ï–ù–¢–ò–†–û–í–ê–ù–ò–Ø
    # ================================

    # === –ü–µ—Ä–≤—ã–π –∑–∞–ø—É—Å–∫? (—Ç–æ–ª—å–∫–æ –ø–µ—Ä–≤–∞—è —Å—Ç—Ä–æ–∫–∞) ===
    try:
        first_row = sheet.row_values(1)
        is_first_run = not any(cell.strip() for cell in first_row)
    except Exception:
        is_first_run = True

    if is_first_run:
        print("üÜï First run: downloading RSS to collect headers...")
        rss_resp = requests.get(RSS_URL, verify=False, timeout=15)
        rss_resp.raise_for_status()
        feed = feedparser.parse(rss_resp.content)
        if not feed.entries:
            print("üì≠ No entries in RSS")
            return
        headers = collect_all_field_names_from_items(feed.entries)
        print(f"üìù Creating header with {len(headers)} columns")
        sheet.update(range_name='A1', values=[headers])
        first_row = headers
    else:
        first_row = sheet.row_values(1)
        print(f"üìù Read first line Type: {type(first_row)} Value {first_row}")

    headers = first_row
    header_to_col = {name: i for i, name in enumerate(headers)}
    required_cols = ["–ö–∞–¥–∞—Å—Ç—Ä–æ–≤—ã–π –Ω–æ–º–µ—Ä", "Nspd_data", "Nspd_error", "Unsorted"]
    for col in required_cols:
        if col not in header_to_col:
            raise RuntimeError(f"Missing required column: {col}")


    # === –ù–∞—Ö–æ–¥–∏–º –ø–æ—Å–ª–µ–¥–Ω—é—é –¥–∞—Ç—É –ø—É–±–ª–∏–∫–∞—Ü–∏–∏ (—ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ) ===
    pubdate_col_name = normalize_field_name("pubDate")
    last_pub_date = None
    if pubdate_col_name in header_to_col:
        col_idx = header_to_col[pubdate_col_name]
        col_letter = gspread.utils.rowcol_to_a1(1, col_idx + 1)[0]  # 'A', 'B', ...

        last_row = find_last_filled_row_in_column(sheet, col_letter)
        if last_row > 0:
            range_name = f"{col_letter}{last_row}:{col_letter}{last_row}"
            try:
                values = sheet.get(range_name)
                if values and values[0] and values[0][0].strip():
                    date_str = values[0][0].strip()
                    try:
#                        last_pub_date = datetime.fromisoformat(date_str.replace("Z", "+00:00"))
                        last_pub_date = parse_date_flexible(date_str)
                    except Exception as e:
                        print(f"‚ö†Ô∏è Invalid date format in row {last_row}: {date_str} ({e})")
            except Exception as e:
                print(f"‚ö†Ô∏è Could not read date from row {last_row}: {e}")
        else:
            print("üì≠ No pubDate entries found in sheet")
    else:
        print("‚ö†Ô∏è Column 'Pubdate' not found in headers")

    print(f"üïó Last processed pubDate: {last_pub_date}")

    # === –ü–æ–≤—Ç–æ—Ä–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –æ—à–∏–±–æ–∫ (–ø–æ—Å–ª–µ–¥–Ω–∏–µ 100 —Å—Ç—Ä–æ–∫) ===
    cad_col_idx = header_to_col["–ö–∞–¥–∞—Å—Ç—Ä–æ–≤—ã–π –Ω–æ–º–µ—Ä"]
    error_col_idx = header_to_col["Nspd_error"]
    geo_col_idx = header_to_col["Nspd_data"]
    cad_col_letter = gspread.utils.rowcol_to_a1(1, cad_col_idx + 1)[0]
    error_col_letter = gspread.utils.rowcol_to_a1(1, error_col_idx + 1)[0]

    rows_to_update = []
    total_rows = sheet.row_count
    if total_rows >= 2:
        start_row = max(2, total_rows - 99)
        try:
            cad_vals = sheet.get(f"{cad_col_letter}{start_row}:{cad_col_letter}") or []
            err_vals = sheet.get(f"{error_col_letter}{start_row}:{error_col_letter}") or []
            session = get_session_with_cookies()
            for i in range(len(cad_vals)):
                row_num = start_row + i
                cad = cad_vals[i][0].strip() if i < len(cad_vals) and cad_vals[i] else ""
                err = err_vals[i][0].strip() if i < len(err_vals) and err_vals[i] else ""
                if cad and CADASTRAL_PATTERN.fullmatch(cad) and err:
                    print(f"üîÅ Retrying {cad} (row {row_num})")
                    geo_data, error = fetch_geoportal_data(session, cad)
                    if error is None:
                        geo_str = json.dumps(geo_data, ensure_ascii=False)
                        rows_to_update.append({"range": gspread.utils.rowcol_to_a1(row_num, geo_col_idx + 1), "values": [[geo_str]]})
                        rows_to_update.append({"range": gspread.utils.rowcol_to_a1(row_num, error_col_idx + 1), "values": [[""]]})
                    time.sleep(0.5)
        except Exception as e:
            print(f"‚ö†Ô∏è Error during retry: {e}")

    if rows_to_update:
        print(f"üì§ Updating {len(rows_to_update)} rows")
        for upd in rows_to_update:
            sheet.update(range_name=upd["range"], values=upd["values"])

    # === –û–±—Ä–∞–±–æ—Ç–∫–∞ –Ω–æ–≤—ã—Ö –ª–æ—Ç–æ–≤ –∏–∑ RSS ===
    print("üîç Fetching RSS for new lots...")
    rss_resp = requests.get(RSS_URL, verify=False, timeout=15)
    rss_resp.raise_for_status()
    feed = feedparser.parse(rss_resp.content)
    if not feed.entries:
        print("üì≠ No RSS entries")
        return

    rss_items = []
    for item in feed.entries:
        pub_dt = None
        if hasattr(item, 'published_parsed') and item.published_parsed:
            try:
                pub_dt = datetime.fromtimestamp(time.mktime(item.published_parsed))
            except:
                pass
        rss_items.append((pub_dt, item))
    rss_items.sort(key=lambda x: x[0] or datetime.min)

    new_rows = []
    for pub_dt, item in rss_items:
        if pub_dt and last_pub_date and pub_dt <= last_pub_date:
            continue

        item_fields = extract_item_raw_fields(item)
        desc_fields = parse_description_fields(item_fields.get("description", ""))
        cad_num = extract_cadastral_number_from_item(item_fields, desc_fields)

        # ‚úÖ –ü–æ–ª—É—á–∞–µ–º Lot_info
        lot_info = ""
        link_val = item_fields.get("link", "")
        if link_val:
            lot_id = extract_lot_id_from_link(link_val)
            if lot_id:
                lot_data = fetch_lot_info(lot_id, link_val)
                if lot_data:
                    lot_info = json.dumps(lot_data, ensure_ascii=False)

        nspd_data, nspd_error = "", ""
        if cad_num:
            session = get_session_with_cookies()  # –∏–ª–∏ –ø–µ—Ä–µ–∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ —Å–µ—Å—Å–∏—é
            geo_data, error = fetch_geoportal_data(session, cad_num)
            if error is None:
                nspd_data = json.dumps(geo_data, ensure_ascii=False)
            else:
                nspd_error = error

        row = build_row_for_sheet(
            item_fields=item_fields,
            desc_fields=desc_fields,
            headers=headers,
            cadastral_number=cad_num,
            nspd_data=nspd_data,
            nspd_error=nspd_error
        )

        # –î–æ–±–∞–≤–ª—è–µ–º Lot_info –≤—Ä—É—á–Ω—É—é (—Ç.–∫. –µ–≥–æ –Ω–µ—Ç –≤ item_fields)
        if LOT_INFO_COL in header_to_col:
            row[header_to_col[LOT_INFO_COL]] = lot_info

        new_rows.append(row)
        time.sleep(0.5)

#    if new_rows:
#        print(f"‚úÖ Appending {len(new_rows)} new rows")
#        sheet.append_rows(new_rows)
    if new_rows:
        print(f"‚úÖ Appending {len(new_rows)} new rows")
        validated_rows = []
        for i, row in enumerate(new_rows):
            # –ü–æ–ø—ã—Ç–∞–µ–º—Å—è –ø–æ–ª—É—á–∏—Ç—å lot_id –∏–∑ —Å—Ç—Ä–æ–∫–∏ (–Ω–∞–ø—Ä–∏–º–µ—Ä, –∏–∑ –∫–æ–ª–æ–Ω–∫–∏ Link)
            lot_id = ""
            try:
                link_col_name = normalize_field_name("link")
                if link_col_name in header_to_col:
                    link_val = row[header_to_col[link_col_name]]
                    lot_id = extract_lot_id_from_link(link_val)
            except:
                pass
    
            validated_row = validate_and_truncate_row(
                row=row,
                headers=headers,
                row_index_in_batch=i + 1,  # –Ω—É–º–µ—Ä–∞—Ü–∏—è —Å 1
                lot_id=lot_id
            )
            validated_rows.append(validated_row)
    
        sheet.append_rows(validated_rows)
    else:
        print("üì≠ No new lots.")

if __name__ == "__main__":
    main()